General Design Notes and Musings


=== Local and/or Remote Mode ===
The system was designed to run both locally and remote and to fall back automatically when an internet connection is lost or does not exist. It will also auto recover if the connection is reestablished. The system can be configured to run in local only mode if so desired. You would only do this for performance reasons when strictly using local TTS, STT and NLP because when in remote mode local mode will be delayed until the remote has timed out. This is not the case when configured in 'local only' mode.

One of the system design goals was a small footprint so when configured in 'remote only' mode the code can run on nearly any device which runs linux.


=== AEC Concerns ===
When configured in remote mode, all utterances are processed by the STT engine using both the local and remote services. Since the remote is often more reliable (for example in an 'edge' type environment), a key value store is persisted and used in the future when only a local response is available, to favor the previous remote response over the new local response. 

Since smaller cheaper systems tend to have poor quality AEC (if any at all) the concept of 'logical AEC' is introduced. The concept of 'logical AEC' is kind of explained below  :-)

It is often the case that cheaper computer audio systems (like a laptop) struggle with AEC. AEC is automatic echo cancellation and it is basically the process of removing the speaker output from the microphone input. This can be challenging for several reasons (buffer alignment is one) and so this system assumes nothing and does several 'logical level things' to work around this sometimes inherent system limitation.

For example, one cause of echo in an input signal could be the TTS previously generated by the system itself when picked up by the microphone. To combat this text 'bleedover', the system remembers the last TTS chunk it rendered (in text format) and when all else fails from a recognition perspective, removes it from the STT input stream. 

Another example would be since the current audio input implementation uses the filesystem as a LIFO the system clears this directly, immediately before a raw (converse) request is initiated. The same for the STT LIFO. 

Out of band (OOB) handling of the 'stop' aliases (stop, terminate, cancel, etc) improves barge-in detection. Isolated wake word recognition (where you just say the wake word) causes the system to beep (audio confirmation) and pause the output until an utterance is received. This is another method employed to overcome poor AEC. In fact, one could argue this is endemic in the input handling portion of the framework (see mic.py, stt.py and intent.py) and could be removed entirely for systems with good AEC. For example, none of these issues exist when I use my wireless headset with any system.


=== Message Bus ===
The message bus was implemented as a websocket server by design so skills do not have to live on the same machine and they may be written in any programming language that supports the websocket protocol. For example, Javascript or Ruby or PHP, etc. It is by design a 'directed message broker' with support for broadcast messages.


=== Compatability ===
This latest version of the code has been verified on the following environments
Circa 2012 MacBook Pro 8 core i7 running Ubuntu 20.x 8GB RAM
Raspberry Pi4 running Pi OS 64Bit, 2GB RAM (using wireless USB Headset)
Raspberry Pi4 running Pi OS 32Bit, 2GB RAM (using wireless USB Headset)
Raspberry Pi4 running Ubuntu 64Bit, 8GB RAM (using wireless USB Headset)
Mycroft Mark II running LINUX Pantacor build 2GB RAM using SJ201
Jetson Nano running NVIDIA Linux 4GB RAM (using wireless USB Headset)
Scott's Beast with his monster GPU :-)

